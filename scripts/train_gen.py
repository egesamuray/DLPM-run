#!/usr/bin/env python3
"""
train_gen.py â€“ Stand-alone DLPM trainer + sampler
for 512Ã—256 grayscale seismic slices.

â€¢ disables evaluation metrics (no pyemd needed)
â€¢ trains for --epochs full passes of the dataset
â€¢ always samples from EMA weights if they exist
"""

import argparse, math, sys, types, torch
from pathlib import Path
import torchvision.utils as tvu

# â”€â”€ stub out pyemd (metric library weâ€™re not using) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sys.modules["pyemd"] = types.ModuleType("pyemd")   # empty stub

# â”€â”€ import local packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(ROOT))

import bem.Experiments as Exp
from bem.utils_exp import FileHandler
import dlpm.dlpm_experiment as dlpm_exp

# â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
P = argparse.ArgumentParser()
P.add_argument("--config",   default="seismic_rect_safe.yml",
               help="YAML in dlpm/configs/")
P.add_argument("--name",     default="seismic_exp_safe",
               help="sub-folder under models/ for checkpoints+png")
P.add_argument("--epochs",   type=int, default=1200,
               help="full dataset passes (â‰ˆ steps / 234)")
P.add_argument("--batch",    type=int, default=16)
P.add_argument("--generate", type=int, default=16)
P.add_argument("--steps",    type=int, default=1000,
               help="reverse diffusion steps at sampling")
P.add_argument("--progress", action="store_true")
args = P.parse_args()

# â”€â”€ load & patch YAML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cfg = ROOT / "dlpm" / "configs" / args.config
params = FileHandler.get_param_from_config(cfg.parent, cfg.name)

params["run"].update(dict(
    eval_freq       = None,            # no FID/KID/EMD
    checkpoint_freq = args.epochs + 1, # save only final
    epochs          = args.epochs
))
params["training"]["batch_size"]  = args.batch
params["training"]["num_workers"] = params["training"].get("num_workers", 0)
params["exp"]["name"]             = args.name

# â”€â”€ build experiment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
exp = Exp.Experiment(
        checkpoint_dir = ROOT / "models" / args.name,
        p              = params,
        logger         = None,
        exp_hash       = dlpm_exp.exp_hash,
        eval_hash      = None,
        init_method_by_parameter        = dlpm_exp.init_method_by_parameter,
        init_models_by_parameter        = dlpm_exp.init_models_by_parameter,
        init_optimizers_by_parameter    = dlpm_exp.init_optimizers_by_parameter,
        init_schedulers_by_parameter    = dlpm_exp.init_schedulers_by_parameter,
        reset_models                    = dlpm_exp.reset_models)
exp.prepare()

# â”€â”€ training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nðŸŸ¢  Training for {args.epochs} epochs â€¦\n")
exp.run(no_ema_eval=True, progress=args.progress)
exp.save(exp.manager.epoch)      # final checkpoint
print("âœ”  training done.")

# â”€â”€ unconditional generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nðŸŸ¢  Generating {args.generate} samples â€¦\n")
gen_man = exp.manager.eval.gen_manager

# Select EMA weights if available
if getattr(exp.manager, "ema_objects", None):
    ema = exp.manager.ema_objects[0]
    models = {k: ema[k].get_ema_model().to(device).eval() for k in exp.manager.models}
else:
    models = {k: m.to(device).eval() for k, m in exp.manager.models.items()}

# ---------------------------------------------------------------------
#  Disable gradients during sampling  â—„ NEW
# ---------------------------------------------------------------------
with torch.no_grad():
    gen_man.generate(models, args.generate,
                     deterministic=False,
                     reverse_steps=args.steps,
                     print_progression=True)

# ---------------------------------------------------------------------
#  Save as 0â€‘1 PNG (do **not** renormalise)  â—„ NEW
# ---------------------------------------------------------------------
grid_rows = max(1, int(math.sqrt(args.generate)))
out_png   = ROOT / f"samples_{args.generate}.png"
tvu.save_image(gen_man.samples, out_png, nrow=grid_rows)

print(f"âœ”  samples saved â†’ {out_png.resolve()}\nðŸŽ‰  Done.")
